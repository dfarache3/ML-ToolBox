{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States: A state represents the current situation or configuration of the environment\n",
    "# s of S (all states)\n",
    "import numpy as np\n",
    "\n",
    "# Create object of 5x5 grid with objective to from start to end\n",
    "# Goal is (4,4) start is (0,0)\n",
    "class GridWorld:\n",
    "    def __init__(self, width: int = 5, height: int = 5, start: tuple = (0, 0), goal: tuple = (4, 4), obstacles: list = None):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.start = np.array(start)\n",
    "        self.goal = np.array(goal)\n",
    "        self.obstacles = [np.array(obstacle) for obstacle in obstacles] if obstacles else []\n",
    "        self.state = self.start\n",
    "\n",
    "\n",
    "# Actions: Actions are the choices available to the agent that can change the state.\n",
    "# a of A (all actions)\n",
    "\n",
    "# Define plausible actions/moves\n",
    "action_effects = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
    "\n",
    "# Check for boundaries and obstacles\n",
    "if 0 <= next_state[0] < self.height and 0 <= next_state[1] < self.width and next_state not in self.obstacles:\n",
    "    self.state = next_state\n",
    "\n",
    "\n",
    "# Rewards: Rewards are immediate feedback received from the environment following an action.\n",
    "# R(s, a, s') (s': new state)\n",
    "\n",
    "reward = 100 if (self.state == self.goal).all() else -1\n",
    "\n",
    "\n",
    "# Episodes: An episode in reinforcement learning is a sequence of steps that starts in an initial state \n",
    "# and ends when a terminal state is reached.\n",
    "# (S_0, A_0, R_1, S_1, A_1, ..., S_(T-1), A_(T-1), R_T, S_T) (T is time)\n",
    "\n",
    "# Policy: A policy is the strategy that an RL agent employs to decide which actions to take in various states.\n",
    "\n",
    "# Deterministic Policy: pickes same action for given state\n",
    "# a = pi(s) (action based on state)\n",
    "\n",
    "# Stochastic Policy: sets probablilty of actions on given state\n",
    "# pi(a|s) = P(A_t = a|S_t = s) (liklihood of choosing action a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Formulation of the RL Problem\n",
    "\n",
    "# Objective function: target that the agent is trying to hit by interacting with the environment\n",
    "\n",
    "# Return: total rewards that an agent picks up\n",
    "# G_t = Sum(R_(t+1), R_T)\n",
    "\n",
    "# Discounting: reduces the value of future rewards with a discount factor γ, which is a number between 0 and 1\n",
    "# G = Sum(R_(t+1), γR_(t+2), ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.8 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# grid with width, height, start, goal\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    GridWorld environment for navigation.\n",
    "    \n",
    "    Args:\n",
    "    - width: Width of the grid\n",
    "    - height: Height of the grid\n",
    "    - start: Start position of the agent\n",
    "    - goal: Goal position of the agent\n",
    "    - obstacles: List of obstacles in the grid\n",
    "        \n",
    "    Methods:\n",
    "    - reset: Reset the environment to the start state\n",
    "    - is_valid_state: Check if the given state is valid\n",
    "    - step: Take a step in the environment\n",
    "    \"\"\"\n",
    "    def __init__(self, width: int = 5, height: int = 5, start: tuple = (0, 0), goal: tuple = (4, 4), obstacles: list = None):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.start = np.array(start)\n",
    "        self.goal = np.array(goal)\n",
    "        self.obstacles = [np.array(obstacle) for obstacle in obstacles] if obstacles else []\n",
    "        self.state = self.start\n",
    "\n",
    "        # possible movement (up, down, left, right)\n",
    "        self.actions = {'up': np.array([-1, 0]), 'down': np.array([1, 0]), 'left': np.array([0, -1]), 'right': np.array([0, 1])}\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\" \n",
    "        Reset the environment to the start state\n",
    "        \n",
    "        Returns:\n",
    "        - Start state of the environment\n",
    "        \"\"\"\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "\n",
    "    def is_valid_state(self, state):\n",
    "        \"\"\"\n",
    "        Check if the given state is valid\n",
    "\n",
    "        Args:\n",
    "        - state: State to be checked\n",
    "\n",
    "        Returns:\n",
    "        - True if the state is valid, False otherwise\n",
    "        \"\"\"\n",
    "        return 0 <= state[0] < self.height and 0 <= state[1] < self.width and all((state != obstacle).any() for obstacle in self.obstacles)\n",
    "\n",
    "    def step(self, action: str):\n",
    "        \"\"\"\n",
    "        Take a step in the environment\n",
    "\n",
    "        Args:\n",
    "        - action: Action to be taken\n",
    "\n",
    "        Returns:\n",
    "        - Next state, reward, done\n",
    "        \"\"\"\n",
    "        next_state = self.state + self.actions[action]\n",
    "        if self.is_valid_state(next_state):\n",
    "            self.state = next_state\n",
    "        reward = 100 if (self.state == self.goal).all() else -1\n",
    "        done = (self.state == self.goal).all()\n",
    "        return self.state, reward, done\n",
    "\n",
    "def navigation_policy(state: np.array, goal: np.array, obstacles: list):\n",
    "    \"\"\"\n",
    "    Policy for navigating the agent in the grid world environment\n",
    "\n",
    "    Args:\n",
    "    - state: Current state of the agent\n",
    "    - goal: Goal state of the agent\n",
    "    - obstacles: List of obstacles in the environment\n",
    "\n",
    "    Returns:\n",
    "    - Action to be taken by the agent\n",
    "    \"\"\"\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "    valid_actions = {}\n",
    "    for action in actions:\n",
    "        next_state = state + env.actions[action]\n",
    "        if env.is_valid_state(next_state):\n",
    "            valid_actions[action] = np.sum(np.abs(next_state - goal))\n",
    "    return min(valid_actions, key=valid_actions.get) if valid_actions else None\n",
    "\n",
    "def run_simulation_with_policy(env: GridWorld, policy):\n",
    "    \"\"\"\n",
    "    Run the simulation with the given policy\n",
    "\n",
    "    Args:\n",
    "    - env: GridWorld environment\n",
    "    - policy: Policy to be used for navigation\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    logging.info(f\"Start State: {state}, Goal: {env.goal}, Obstacles: {env.obstacles}\")\n",
    "    while not done:\n",
    "        # Visualization\n",
    "        grid = np.zeros((env.height, env.width))\n",
    "        grid[tuple(state)] = 1  # current state\n",
    "        grid[tuple(env.goal)] = 2  # goal\n",
    "        for obstacle in env.obstacles:\n",
    "            grid[tuple(obstacle)] = -1  # obstacles\n",
    "\n",
    "        plt.imshow(grid, cmap='Pastel1')\n",
    "        plt.show()\n",
    "\n",
    "        action = policy(state, env.goal, env.obstacles)\n",
    "        if action is None:\n",
    "            logging.info(\"No valid actions available, agent is stuck.\")\n",
    "            break\n",
    "        next_state, reward, done = env.step(action)\n",
    "        logging.info(f\"State: {state} -> Action: {action} -> Next State: {next_state}, Reward: {reward}\")\n",
    "        state = next_state\n",
    "        if done:\n",
    "            logging.info(\"Goal reached!\")\n",
    "\n",
    "# Define obstacles in the environment\n",
    "obstacles = [(1, 1), (1, 2), (2, 1), (3, 3)]\n",
    "\n",
    "# Create the environment with obstacles\n",
    "env = GridWorld(obstacles=obstacles)\n",
    "\n",
    "# Run the simulation\n",
    "run_simulation_with_policy(env, navigation_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
