{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize time series data for Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "data = pd.read_csv('Data/PJME_hourly.csv', parse_dates=['Datetime'], index_col='Datetime')\n",
    "\n",
    "# Assuming pjme_data is loaded as before\n",
    "daily_data = data.resample('D').mean() \n",
    "\n",
    "# Prepare data for Prophet\n",
    "daily_data.reset_index(inplace=True)\n",
    "daily_data.columns = ['ds', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize time series data for featurization into a tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reset the datetime\n",
    "data[\"Datetime\"] = data.index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Create copy for multiclass data \n",
    "df = data.copy()\n",
    "\n",
    "# Convert the datetime column\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])  # Adjust the 'datetime' column name as necessary\n",
    "df = df.sort_values('Datetime').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Obtain day and hour\n",
    "df['Date'] = pd.to_datetime(df['Datetime']).dt.floor('D')  \n",
    "df['Hour'] = pd.to_datetime(df['Datetime']).dt.hour\n",
    "\n",
    "# Create multi-index feature df to compute time series features on\n",
    "features = df.set_index(['Date', 'Hour'])  \n",
    "features.drop(\"Datetime\", inplace=True, axis=1)\n",
    "\n",
    "# Split the data into training and testing sets, respecting the temporal order\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, features[\"PJME_MW\"], test_size=0.2, shuffle=False)\n",
    "\n",
    "# Get group lengths\n",
    "train_lengths = X_train.groupby(level=0).size()\n",
    "test_lengths = X_test.groupby(level=0).size()\n",
    "\n",
    "# Obtain common length value for train/test data\n",
    "train_common_length = train_lengths.mode().iloc[0]\n",
    "test_common_length = test_lengths.mode().iloc[0]\n",
    "\n",
    "# Filter train/test data to groups with same common length for featurizer\n",
    "X_train = X_train.groupby(level=0).filter(lambda x: len(x) == train_common_length)\n",
    "X_test = X_test.groupby(level=0).filter(lambda x: len(x) == test_common_length)\n",
    "\n",
    "# Create quartiles based on training data to avoid leakage\n",
    "quartiles = [X_train['PJME_MW'].quantile(q) for q in [0.25, 0.50, 0.75]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Prophet Forecasting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape: (4847, 2)\n",
      "Testing Set Shape: (1212, 2)\n"
     ]
    }
   ],
   "source": [
    "# Cutoff date at 2015-04-09\n",
    "cutoff_index = int(len(daily_data) * 0.8)\n",
    "\n",
    "# Use 80% of data for training set and 20% for test set\n",
    "train_df = daily_data.iloc[:cutoff_index]\n",
    "test_df = daily_data.iloc[cutoff_index:]\n",
    "\n",
    "print(\"Training Set Shape:\", train_df.shape)\n",
    "print(\"Testing Set Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:49:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:49:35 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4266\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize model and train it on training data\n",
    "model = Prophet()\n",
    "model.fit(train_df)\n",
    "\n",
    "# Create a dataframe for future predictions covering the test period\n",
    "future = model.make_future_dataframe(periods=len(test_df), freq='D')\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Categorize forecasted daily values into quartiles based on the thresholds\n",
    "forecast['quartile'] = pd.cut(forecast['yhat'], bins = [-np.inf] + list(quartiles) + [np.inf], labels=[1, 2, 3, 4])\n",
    "\n",
    "# Extract the forecasted quartiles for the test period\n",
    "forecasted_quartiles = forecast.iloc[-len(test_df):]['quartile'].astype(int)\n",
    "\n",
    "# Categorize actual daily values in the test set into quartiles\n",
    "test_df['quartile'] = pd.cut(test_df['y'], bins=[-np.inf] + list(quartiles) + [np.inf], labels=[1, 2, 3, 4])\n",
    "actual_test_quartiles = test_df['quartile'].astype(int)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "prophet_accuracy = accuracy_score(actual_test_quartiles, forecasted_quartiles)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Accuracy: {prophet_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    3\n",
       "2    3\n",
       "3    3\n",
       "4    2\n",
       "Name: quartile, dtype: category\n",
       "Categories (4, int64): [1 < 2 < 3 < 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For illustrative purposes we show the quartiles for training data\n",
    "train_df['quartile'] = pd.cut(train_df['y'], bins = [-np.inf] + list(quartiles) + [np.inf], labels=[1, 2, 3, 4])\n",
    "train_df['quartile'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 4817/4817 [00:00<00:00, 5329.41it/s]\n",
      "Feature Extraction: 100%|██████████| 1205/1205 [00:00<00:00, 4972.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import tsfel\n",
    "from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "\n",
    "# Define tsfresh feature extractor\n",
    "tsfresh_trafo = TSFreshFeatureExtractor(default_fc_parameters=\"minimal\")\n",
    "\n",
    "# Transform the training data using the feature extractor\n",
    "X_train_transformed = tsfresh_trafo.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same feature extractor\n",
    "X_test_transformed = tsfresh_trafo.transform(X_test)\n",
    "\n",
    "# Retrieves a pre-defined feature configuration file to extract all available features\n",
    "cfg = tsfel.get_features_by_domain()\n",
    "\n",
    "# Function to compute tsfel features per day\n",
    "def compute_features(group):\n",
    "    # TSFEL expects a DataFrame with the data in columns, so we transpose the input group\n",
    "    features = tsfel.time_series_features_extractor(cfg, group, fs=1, verbose=0)\n",
    "    return features\n",
    "\n",
    "\n",
    "# Group by the 'day' level of the index and apply the feature computation\n",
    "train_features_per_day = X_train.groupby(level='Date').apply(compute_features).reset_index(drop=True)\n",
    "test_features_per_day = X_test.groupby(level='Date').apply(compute_features).reset_index(drop=True)\n",
    "\n",
    "# Combine each featurization into a set of combined features for our train/test data\n",
    "train_combined_df = pd.concat([X_train_transformed, train_features_per_day], axis=1)\n",
    "test_combined_df = pd.concat([X_test_transformed, test_features_per_day], axis=1)\n",
    "\n",
    "# Filter out features that are highly correlated with our target variable\n",
    "column_of_interest = \"PJME_MW__mean\"\n",
    "train_corr_matrix = train_combined_df.corr()\n",
    "train_corr_with_interest = train_corr_matrix[column_of_interest]\n",
    "null_corrs = pd.Series(train_corr_with_interest.isnull())\n",
    "false_features = null_corrs[null_corrs].index.tolist()\n",
    "\n",
    "columns_to_exclude = list(set(train_corr_with_interest[abs(train_corr_with_interest) > 0.8].index.tolist() + false_features))\n",
    "columns_to_exclude.remove(column_of_interest)\n",
    "\n",
    "# Filtered DataFrame excluding columns with high correlation to the column of interest\n",
    "X_train_transformed = train_combined_df.drop(columns=columns_to_exclude)\n",
    "X_test_transformed = test_combined_df.drop(columns=columns_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify each value into a quartile\n",
    "def classify_into_quartile(value):\n",
    "    if value < quartiles[0]:\n",
    "        return 1  \n",
    "    elif value < quartiles[1]:\n",
    "        return 2  \n",
    "    elif value < quartiles[2]:\n",
    "        return 3  \n",
    "    else:\n",
    "        return 4  \n",
    "\n",
    "y_train = X_train_transformed[\"PJME_MW__mean\"].rename(\"daily_energy_level\")\n",
    "X_train_transformed.drop(\"PJME_MW__mean\", inplace=True, axis=1)\n",
    "\n",
    "y_test = X_test_transformed[\"PJME_MW__mean\"].rename(\"daily_energy_level\")\n",
    "X_test_transformed.drop(\"PJME_MW__mean\", inplace=True, axis=1)\n",
    "\n",
    "energy_levels_train = y_train.apply(classify_into_quartile)\n",
    "energy_levels_test = y_test.apply(classify_into_quartile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate GradientBoostingClassifier Model on multiclass tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=20,\n",
    "    max_features='sqrt',\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbc.fit(X_train_transformed, energy_levels_train)\n",
    "\n",
    "\n",
    "y_pred_gbc = gbc.predict(X_test_transformed)\n",
    "gbc_accuracy = accuracy_score(energy_levels_test, y_pred_gbc)\n",
    "print(f'Accuracy: {gbc_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AutoML to streamline things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'api_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcleanlab_studio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Studio\n\u001b[0;32m----> 3\u001b[0m studio \u001b[38;5;241m=\u001b[39m \u001b[43mStudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m studio\u001b[38;5;241m.\u001b[39mcreate_project(\n\u001b[1;32m      5\u001b[0m     dataset_id\u001b[38;5;241m=\u001b[39menergy_forecasting_dataset,\n\u001b[1;32m      6\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENERGY-LEVEL-FORECASTING\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     label_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaily_energy_level\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m studio\u001b[38;5;241m.\u001b[39mget_model(energy_forecasting_model)\n",
      "File \u001b[0;32m~/anaconda3/envs/timeSeries/lib/python3.8/site-packages/cleanlab_studio/internal/util.py:280\u001b[0m, in \u001b[0;36mtelemetry.<locals>.track.<locals>.tracked_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     user_info[\u001b[39m\"\u001b[39m\u001b[39merror_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(err)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    276\u001b[0m     _ \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcli_base_url\u001b[39m}\u001b[39;00m\u001b[39m/telemetry\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    278\u001b[0m         json\u001b[39m=\u001b[39muser_info,\n\u001b[1;32m    279\u001b[0m     )\n\u001b[0;32m--> 280\u001b[0m \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/anaconda3/envs/timeSeries/lib/python3.8/site-packages/cleanlab_studio/internal/util.py:244\u001b[0m, in \u001b[0;36mtelemetry.<locals>.track.<locals>.tracked_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtracked_func\u001b[39m(\u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    243\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m         result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    245\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    246\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'api_key'"
     ]
    }
   ],
   "source": [
    "from cleanlab_studio import Studio\n",
    "\n",
    "studio = Studio()\n",
    "studio.create_project(\n",
    "    dataset_id=energy_forecasting_dataset,\n",
    "    project_name=\"ENERGY-LEVEL-FORECASTING\",\n",
    "    modality=\"tabular\",\n",
    "    task_type=\"multi-class\",\n",
    "    model_type=\"regular\",\n",
    "    label_column=\"daily_energy_level\",\n",
    ")\n",
    "\n",
    "model = studio.get_model(energy_forecasting_model)\n",
    "y_pred_automl = model.predict(test_data, return_pred_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'quartile-multiclass-pjme-testing-data_pred_probs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred_automl_cleanlab \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquartile-multiclass-pjme-testing-data_pred_probs.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m y_pred_automl_cleanlab \u001b[38;5;241m=\u001b[39m y_pred_automl_cleanlab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuggested Label\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m automl_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(energy_levels_test, y_pred_automl_cleanlab)\n",
      "File \u001b[0;32m~/anaconda3/envs/timeSeries/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/timeSeries/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/timeSeries/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/timeSeries/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/timeSeries/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'quartile-multiclass-pjme-testing-data_pred_probs.csv'"
     ]
    }
   ],
   "source": [
    "y_pred_automl_cleanlab = pd.read_csv(\"quartile-multiclass-pjme-testing-data_pred_probs.csv\")\n",
    "y_pred_automl_cleanlab = y_pred_automl_cleanlab[\"Suggested Label\"]\n",
    "\n",
    "automl_accuracy = accuracy_score(energy_levels_test, y_pred_automl_cleanlab)\n",
    "print(f'Accuracy: {automl_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error rates\n",
    "error_rate_prophet = 1 - prophet_accuracy\n",
    "error_rate_gbc = 1 - gbc_accuracy\n",
    "error_rate_automl = 1 - automl_accuracy\n",
    "\n",
    "print(f\"Prophet error rate: {error_rate_prophet}\")\n",
    "print(f\"GBC error rate: {error_rate_gbc}\")\n",
    "print(f\"AutoML error rate: {error_rate_automl}\")\n",
    "\n",
    "# Calculate reduction in prediction error\n",
    "error_reduction_gbc = error_rate_prophet - error_rate_gbc\n",
    "error_reduction_automl_to_prophet = error_rate_prophet - error_rate_automl\n",
    "error_reduction_automl_to_gbc = error_rate_gbc - error_rate_automl\n",
    "\n",
    "# Convert error reduction to a percentage of improvement from one model to another\n",
    "percentage_improvement_gbc = round((error_reduction_gbc / error_rate_prophet) * 100, 1)\n",
    "percentage_improvement_automl_to_prophet = round((error_reduction_automl_to_prophet / error_rate_prophet) * 100, 1)\n",
    "percentage_improvement_automl_to_gbc = round((error_reduction_automl_to_gbc / error_rate_gbc) * 100, 1)\n",
    "\n",
    "print(f\"GBC compared to Prophet resulted in a {percentage_improvement_gbc}% reduction in prediction error.\")\n",
    "print(f\"AutoML compared to Prophet resulted in a {percentage_improvement_automl_to_prophet}% reduction in prediction error.\")\n",
    "print(f\"AutoML compared to GBC resulted in a {percentage_improvement_automl_to_gbc}% reduction in prediction error.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('timeSeries')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7504e16e27b894bb43197d58c1e324071fae0b8995196fe22e115cf3329a2879"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
