{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import typing\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "NoneFloat = typing.Union[None, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, \n",
    "                dim: int,\n",
    "                chan: int,\n",
    "                num_heads: int=1,\n",
    "                qkv_bias: bool=False,\n",
    "                qk_scale: NoneFloat=None):\n",
    "\n",
    "        \"\"\" Attention Module\n",
    "\n",
    "            Args:\n",
    "                dim (int): input size of a single token\n",
    "                chan (int): resulting size of a single token (channels)\n",
    "                num_heads(int): number of attention heads in MSA\n",
    "                qkv_bias (bool): determines if the qkv layer learns an addative bias\n",
    "                qk_scale (NoneFloat): value to scale the queries and keys by; \n",
    "                                    if None, queries and keys are scaled by ``head_dim ** -0.5``\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        ## Define Constants\n",
    "        self.num_heads = num_heads\n",
    "        self.chan = chan\n",
    "        self.head_dim = self.chan // self.num_heads\n",
    "        self.scale = qk_scale or self.head_dim ** -0.5\n",
    "        assert self.chan % self.num_heads == 0, '\"Chan\" must be evenly divisible by \"num_heads\".'\n",
    "\n",
    "        ## Define Layers\n",
    "        self.qkv = nn.Linear(dim, chan * 3, bias=qkv_bias)\n",
    "        #### Each token gets projected from starting length (dim) to channel length (chan) 3 times (for each Q, K, V)\n",
    "        self.proj = nn.Linear(chan, chan)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        ## Dimensions: (batch, num_tokens, token_len)\n",
    "\n",
    "        ## Calcuate QKVs\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        #### Dimensions: (3, batch, heads, num_tokens, chan/num_heads = head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        ## Calculate Attention\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        #### Dimensions: (batch, heads, num_tokens, num_tokens)\n",
    "\n",
    "        ## Attention Layer\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, self.chan)\n",
    "        #### Dimensions: (batch, heads, num_tokens, chan)\n",
    "\n",
    "        ## Projection Layers\n",
    "        x = self.proj(x)\n",
    "\n",
    "        ## Skip Connection Layer\n",
    "        v = v.transpose(1, 2).reshape(B, N, self.chan)\n",
    "        x = v + x     \n",
    "        #### Because the original x has different size with current x, use v to do skip connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\ttoken size: 49\n",
      "Dimensions for Queries are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 1 \n",
      "\tnumber of tokens: 100 \n",
      "\tnew length of tokens: 64\n",
      "See that the dimensions for queries, keys, and values are all the same:\n",
      "\tShape of Q: torch.Size([13, 1, 100, 64]) \n",
      "\tShape of K: torch.Size([13, 1, 100, 64]) \n",
      "\tShape of V: torch.Size([13, 1, 100, 64])\n"
     ]
    }
   ],
   "source": [
    "# Define an Input\n",
    "token_len = 7*7\n",
    "channels = 64\n",
    "num_tokens = 100\n",
    "batch = 13\n",
    "x = torch.rand(batch, num_tokens, token_len)\n",
    "B, N, C = x.shape\n",
    "print('Input dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\n",
    "\n",
    "# Define the Module\n",
    "A = Attention(dim=token_len, chan=channels, num_heads=1, qkv_bias=False, qk_scale=None)\n",
    "A.eval();\n",
    "\n",
    "qkv = A.qkv(x).reshape(B, N, 3, A.num_heads, A.head_dim).permute(2, 0, 3, 1, 4)\n",
    "q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "print('Dimensions for Queries are\\n\\tbatchsize:', q.shape[0], '\\n\\tattention heads:', q.shape[1], '\\n\\tnumber of tokens:', q.shape[2], '\\n\\tnew length of tokens:', q.shape[3])\n",
    "print('See that the dimensions for queries, keys, and values are all the same:')\n",
    "print('\\tShape of Q:', q.shape, '\\n\\tShape of K:', k.shape, '\\n\\tShape of V:', v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention(Q, K, V) = softmax((Q*K^T) / sqrt(d_k)) * V \n",
    "\n",
    "A = Q*K^T/sqrt(d_k) \n",
    "\n",
    "qk_scale = 1/sqrt(d_k) = chan^(-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions for Attn are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 1 \n",
      "\tnumber of tokens: 100 \n",
      "\tnumber of tokens: 100\n",
      "Dimensions for Attn are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 1 \n",
      "\tnumber of tokens: 100 \n",
      "\tnumber of tokens: 100\n"
     ]
    }
   ],
   "source": [
    "# Calculate attention \n",
    "attn = (q * A.scale) @ k.transpose(-2, -1)\n",
    "print('Dimensions for Attn are\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])\n",
    "\n",
    "attn = attn.softmax(dim=-1)\n",
    "print('Dimensions for Attn are\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions for x are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 1 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 64\n",
      "Dimensions for x are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 64\n",
      "Dimensions for x are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 64\n"
     ]
    }
   ],
   "source": [
    "# Do Attention times V\n",
    "x = attn @ v\n",
    "print('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tattention heads:', x.shape[1], '\\n\\tnumber of tokens:', x.shape[2], '\\n\\tlength of tokens:', x.shape[3])\n",
    "\n",
    "# Output is reshaped to remove the attention head dimension.\n",
    "x = x.transpose(1, 2).reshape(B, N, A.chan)\n",
    "print('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])\n",
    "\n",
    "# Feed through learnable linear layer that does not change it's shape\n",
    "x = A.proj(x)\n",
    "print('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of input x: (13, 100, 49)\n",
      "Current shape of x: (13, 100, 64)\n",
      "Shape of V: (13, 100, 64)\n",
      "After skip connection, dimensions for x are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 64\n"
     ]
    }
   ],
   "source": [
    "# Implement skip connection\n",
    "orig_shape = (batch, num_tokens, token_len)\n",
    "curr_shape = (x.shape[0], x.shape[1], x.shape[2])\n",
    "\n",
    "# V for skip connection since x.shape != x_input.shape\n",
    "v = v.transpose(1, 2).reshape(B, N, A.chan)\n",
    "v_shape = (v.shape[0], v.shape[1], v.shape[2])\n",
    "print('Original shape of input x:', orig_shape)\n",
    "print('Current shape of x:', curr_shape)\n",
    "print('Shape of V:', v_shape)\n",
    "x = v + x     \n",
    "print('After skip connection, dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi_Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\ttoken size: 49\n"
     ]
    }
   ],
   "source": [
    "# Define an Input\n",
    "token_len = 7*7\n",
    "channels = 64\n",
    "num_tokens = 100\n",
    "batch = 13\n",
    "num_heads = 4 # number of attention heads must evenly divide the number of channels\n",
    "x = torch.rand(batch, num_tokens, token_len)\n",
    "B, N, C = x.shape\n",
    "print('Input dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\n",
    "\n",
    "# Define the Module\n",
    "MSA = Attention(dim=token_len, chan=channels, num_heads=num_heads, qkv_bias=False, qk_scale=None)\n",
    "MSA.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head Dimension = chan / num_heads = 64 / 4 = 16\n",
      "Dimensions for Queries are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 4 \n",
      "\tnumber of tokens: 100 \n",
      "\tnew length of tokens: 16\n",
      "See that the dimensions for queries, keys, and values are all the same:\n",
      "\tShape of Q: torch.Size([13, 4, 100, 16]) \n",
      "\tShape of K: torch.Size([13, 4, 100, 16]) \n",
      "\tShape of V: torch.Size([13, 4, 100, 16])\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V for multi-head = single-head\n",
    "# token length changes to channels / num_heads\n",
    "qkv = MSA.qkv(x).reshape(B, N, 3, MSA.num_heads, MSA.head_dim).permute(2, 0, 3, 1, 4)\n",
    "q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "print('Head Dimension = chan / num_heads =', MSA.chan, '/', MSA.num_heads, '=', MSA.head_dim)\n",
    "print('Dimensions for Queries are\\n\\tbatchsize:', q.shape[0], '\\n\\tattention heads:', q.shape[1], '\\n\\tnumber of tokens:', q.shape[2], '\\n\\tnew length of tokens:', q.shape[3])\n",
    "print('See that the dimensions for queries, keys, and values are all the same:')\n",
    "print('\\tShape of Q:', q.shape, '\\n\\tShape of K:', k.shape, '\\n\\tShape of V:', v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A_hi = (Q_hi * K_hi^T) / sqrt(d_k)\n",
    "\n",
    "d_k = chan / num_heads\n",
    "\n",
    "qk_scale = 1 / sqrt(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions for Attn are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 4 \n",
      "\tnumber of tokens: 100 \n",
      "\tnumber of tokens: 100\n",
      "Dimensions for x are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 4 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 16\n",
      "Dimensions for x are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 64\n"
     ]
    }
   ],
   "source": [
    "# Calc attention for multi-head\n",
    "attn = (q * MSA.scale) @ k.transpose(-2, -1)\n",
    "print('Dimensions for Attn are\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])\n",
    "\n",
    "attn = attn.softmax(dim=-1)\n",
    "\n",
    "x = attn @ v\n",
    "print('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tattention heads:', x.shape[1], '\\n\\tnumber of tokens:', x.shape[2], '\\n\\tlength of tokens:', x.shape[3])\n",
    "\n",
    "# Reshape\n",
    "x = x.transpose(1, 2).reshape(B, N, MSA.chan)\n",
    "print('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement  skip connection, use V, but reshape it to remove the head dimension\n",
    "x = MSA.proj(x)\n",
    "print('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])\n",
    "\n",
    "orig_shape = (batch, num_tokens, token_len)\n",
    "curr_shape = (x.shape[0], x.shape[1], x.shape[2])\n",
    "v = v.transpose(1, 2).reshape(B, N, A.chan)\n",
    "v_shape = (v.shape[0], v.shape[1], v.shape[2])\n",
    "print('Original shape of input x:', orig_shape)\n",
    "print('Current shape of x:', curr_shape)\n",
    "print('Shape of V:', v_shape)\n",
    "x = v + x     \n",
    "print('After skip connection, dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 ('ViT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6baf2752bb02b131252d555bfa97b1b5c089af64a8b0f028036e9aa8b19c365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
